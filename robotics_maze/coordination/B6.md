# B6 Progress Log

## 2026-02-26

### Completed

- Implemented `src/benchmark.py` benchmark harness for multi-maze planner comparisons.
- Integrated benchmark generation with B2 maze APIs (`src/maze.py`) instead of synthetic local mazes.
- Added wall-maze to occupancy-grid conversion for planner interoperability.
- Added planner discovery:
  - Baselines from `src/planners.py`
  - Alternative planners from `src/alt_planners/r1..r9`
- Added per-trial metric capture: solve time, path length, expansions, success/failure, and error text.
- Added report writers:
  - CSV output: `results/benchmark_results.csv`
  - Markdown output: `results/benchmark_summary.md`
- Added lightweight smoke tests in `tests/test_core.py` for:
  - Maze generation determinism/open endpoints.
  - A* solvability assumptions across generated mazes.
  - Benchmark output file generation.
- Documented benchmark usage and output schema in `results/README.md`.

### Notes

- Harness is deterministic via `maze_seed = base_seed + maze_index`.
- Planner outputs are normalized across heterogeneous return schemas (dict / tuple).
- Success requires a valid path from normalized start to normalized goal.
- Task 11 (Planner benchmark UX): improved comparison readability in both terminal and markdown summaries.
  - Added explicit planner ranking policy and rank column.
  - Added delta-time column (`Delta vs #1`) to make relative speed comparisons obvious.
  - Replaced line-by-line terminal summary with an aligned comparison table.

### Blockers

- None from B6 scope.
