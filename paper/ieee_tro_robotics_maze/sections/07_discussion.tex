\section{Discussion}

The current repository snapshot provides a reproducible baseline for grid-maze navigation with measurable efficiency tradeoffs. The benchmark artifact reports 12 planners evaluated on 50 generated $15\times15$ mazes, with 100\% success for every planner in this static setting. Under the implemented ranking policy, \texttt{r1\_weighted\_astar} achieves the lowest mean solve time (0.35 ms), while \texttt{r4\_idastar} shows the highest computational burden (22.56 ms), highlighting the cost of deep iterative search in this map class.

\subsection{What the Current Results Establish}

Three descriptive findings are consistent within the present benchmark regime.
First, weighted heuristic guidance is already beneficial: \texttt{r1\_weighted\_astar} is approximately 35\% faster than baseline \texttt{astar} while keeping equivalent mean path length in this dataset.
Second, frontier-pruning methods reduce search effort but do not always dominate runtime: \texttt{r5\_jump\_point\_search} has the lowest mean expansions (57.26) yet is not the fastest due to additional bookkeeping overhead.
Third, algorithm classes optimized for other settings underperform in this static benchmark; for example, \texttt{r6\_lpa\_star} and \texttt{r4\_idastar} add overhead without gains when maps are regenerated rather than incrementally repaired.
These observations are descriptive summaries from the executed benchmark snapshot and are not
presented as inferential superiority claims.

Beyond planner speed, the infrastructure contributes practical value.
The simulator supports explicit backend selection (\texttt{pybullet}, \texttt{mujoco}, or \texttt{auto}), URDF fallback behavior, and deterministic screenshot regression capture for visual checks.
These implementation details improve reproducibility and reduce benchmark fragility across machines.

\subsection{Implemented Versus Not-Yet-Implemented Methods}

\textbf{Implemented and used in the reported benchmark.}
\begin{itemize}
    \item Baseline grid planners: \texttt{astar}, \texttt{dijkstra}, \texttt{greedy\_best\_first}.
    \item Alternative planners: \texttt{r1\_weighted\_astar}, \texttt{r2\_bidirectional\_astar}, \texttt{r3\_theta\_star}, \texttt{r4\_idastar}, \texttt{r5\_jump\_point\_search}, \texttt{r6\_lpa\_star}, \texttt{r7\_beam\_search}, \texttt{r8\_fringe\_search}, and \texttt{r9\_bidirectional\_bfs}.
    \item Runtime stack support: backend auto-resolution, robust URDF fallback, and deterministic screenshot regression scripts.
\end{itemize}

\textbf{Implemented in repository but not part of the current summary table.}
\begin{itemize}
    \item Additional adapter modules \texttt{r11\_dijkstra}, \texttt{r12\_bfs}, and \texttt{r13\_greedy\_best\_first} exist, but the current benchmark artifact reports the baseline/alternative set above.
\end{itemize}

\textbf{Not yet implemented (currently documented as future directions).}
\begin{itemize}
    \item Learned local heuristics for search (LoHA-style A* extensions).
    \item D* Lite + DWA and AD* + DWA hybrid global-local replanning.
    \item Heading-aware SE(2) Hybrid-A* / state-lattice planning and IGHA* style incremental hybrid search.
    \item Guided RRT* variants and uncertainty-aware MPC for dynamic, continuous navigation.
\end{itemize}

\subsection{Limitations and Threats to Validity}

The present evidence is strong for reproducible static-grid comparison, but limited for broader robotics claims.
\begin{itemize}
    \item \textbf{Task distribution}: all reported mazes are $15\times15$ and generated by one algorithm family in one snapshot, so topology diversity is limited.
    \item \textbf{Metric saturation}: with 100\% success across planners, failure robustness cannot be ranked; only efficiency metrics differentiate methods.
    \item \textbf{Path comparability}: any-angle methods (e.g., \texttt{r3\_theta\_star}) are not directly comparable to strict lattice paths without additional smoothness/feasibility metrics.
    \item \textbf{Physics realism}: MuJoCo fallback currently executes a simplified waypoint progression, and no dynamic-obstacle uncertainty benchmark is yet integrated.
    \item \textbf{Generalization}: current conclusions are for occupancy-grid planning and do not yet establish superiority in SE(2)/kinodynamic domains.
\end{itemize}

\subsection{Future Work}

The next implementation priorities are: (i) learned local heuristics with admissibility safeguards, (ii) incremental global-local replanning under dynamic disturbances, (iii) heading-aware SE(2) planners for executable trajectory quality, and (iv) uncertainty-aware continuous planners for large-map robustness. All four directions require the same acceptance discipline before being promoted into default comparisons: multi-generator and multi-seed evaluation, repeated-run timing protocols, and feasibility-oriented metrics beyond grid-path length.

These items are intentionally framed as future directions rather than current contributions. The present manuscript claims are explicitly bounded to the static-grid benchmark infrastructure and its associated repository-grounded evidence.
