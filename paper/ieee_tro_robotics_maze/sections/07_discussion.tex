\section{Discussion}

The current repository snapshot establishes a reproducible baseline for grid-maze navigation with measurable algorithmic tradeoffs. The benchmark artifact reports 12 planners evaluated on 50 generated $15\times15$ mazes, with 100\% success for every planner in this static setting. Under the executable ranking policy (success rate, then comparable solve time on shared-success mazes, then mean expansions, then overall mean solve time, then planner-name tie-break), \texttt{r1\_weighted\_astar} is first (0.35 ms mean solve time), while \texttt{r4\_idastar} is last (22.56 ms), showing that admissible but deep iterative search remains expensive for this map class.

\subsection{What the Current Results Establish}

Three descriptive findings are consistent within the present benchmark regime.
First, weighted heuristic guidance is already beneficial: \texttt{r1\_weighted\_astar} is approximately 35\% faster than baseline \texttt{astar} while keeping equivalent mean path length in this dataset.
Second, frontier-pruning methods reduce search effort but do not always dominate runtime: \texttt{r5\_jump\_point\_search} has the lowest mean expansions (57.26) yet is not the fastest due to additional bookkeeping overhead.
Third, algorithm classes optimized for other settings underperform in this static benchmark; for example, \texttt{r6\_lpa\_star} and \texttt{r4\_idastar} add overhead without gains when maps are regenerated rather than incrementally repaired.
These observations are descriptive summaries from the executed benchmark snapshot and are not
presented as inferential superiority claims.

Beyond planner speed, the infrastructure contributes practical value.
The simulator supports explicit backend selection (\texttt{pybullet}, \texttt{mujoco}, or \texttt{auto}), URDF fallback behavior, and deterministic screenshot regression capture for visual checks.
These implementation details improve reproducibility and reduce benchmark fragility across machines.

\subsection{Implemented Versus Not-Yet-Implemented Methods}

\textbf{Implemented and used in the reported benchmark.}
\begin{itemize}
    \item Baseline grid planners: \texttt{astar}, \texttt{dijkstra}, \texttt{greedy\_best\_first}.
    \item Alternative planners: \texttt{r1\_weighted\_astar}, \texttt{r2\_bidirectional\_astar}, \texttt{r3\_theta\_star}, \texttt{r4\_idastar}, \texttt{r5\_jump\_point\_search}, \texttt{r6\_lpa\_star}, \texttt{r7\_beam\_search}, \texttt{r8\_fringe\_search}, and \texttt{r9\_bidirectional\_bfs}.
    \item Runtime stack support: backend auto-resolution, robust URDF fallback, and deterministic screenshot regression scripts.
\end{itemize}

\textbf{Implemented in repository but not part of the current summary table.}
\begin{itemize}
    \item Additional adapter modules \texttt{r11\_dijkstra}, \texttt{r12\_bfs}, and \texttt{r13\_greedy\_best\_first} exist, but the current benchmark artifact reports the baseline/alternative set above.
\end{itemize}

\textbf{Not yet implemented (currently documented as future directions).}
\begin{itemize}
    \item Learned local heuristics for search (LoHA-style A* extensions).
    \item D* Lite + DWA and AD* + DWA hybrid global-local replanning.
    \item Heading-aware SE(2) Hybrid-A* / state-lattice planning and IGHA* style incremental hybrid search.
    \item Guided RRT* variants and uncertainty-aware MPC for dynamic, continuous navigation.
\end{itemize}

\subsection{Limitations and Threats to Validity}

The present evidence is strong for reproducible static-grid comparison, but limited for broader robotics claims.
\begin{itemize}
    \item \textbf{Task distribution}: all reported mazes are $15\times15$ and generated by one algorithm family in one snapshot, so topology diversity is limited.
    \item \textbf{Metric saturation}: with 100\% success across planners, failure robustness cannot be ranked; only efficiency metrics differentiate methods.
    \item \textbf{Path comparability}: any-angle methods (e.g., \texttt{r3\_theta\_star}) are not directly comparable to strict lattice paths without additional smoothness/feasibility metrics.
    \item \textbf{Physics realism}: MuJoCo fallback currently executes a simplified waypoint progression, and no dynamic-obstacle uncertainty benchmark is yet integrated.
    \item \textbf{Generalization}: current conclusions are for occupancy-grid planning and do not yet establish superiority in SE(2)/kinodynamic domains.
\end{itemize}

\subsection{Future Work}

The next implementation priorities are: (i) learned local heuristics with admissibility safeguards, (ii) incremental global-local replanning under dynamic disturbances, (iii) heading-aware SE(2) planners for executable trajectory quality, and (iv) uncertainty-aware continuous planners for large-map robustness. All four directions require the same acceptance discipline before being promoted into default comparisons: multi-generator and multi-seed evaluation, repeated-run timing protocols, and feasibility-oriented metrics beyond grid-path length.

These items are intentionally framed as planned work rather than current contributions. The present manuscript claims are limited to the executed static-grid benchmark infrastructure and its repository-grounded evidence.
